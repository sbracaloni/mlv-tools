{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract train and test data set from 20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline step is based on the 01_Extract_dataset.ipynb Jupiter Notebook. It loads\n",
    " the input data (downloaded during the setup steps) and it splits it into 2 datasets: one to train and one to test the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Thanks to mlvtool a Python script and a DVC command will be generated from this notebook. This way it will be easier to version\n",
    "and execute the classifier pipeline. The work/research can still be done in the notebook and then automatically replicated into the script.\n",
    "\n",
    "The following cell (the first code cell) contains a Docstring to describe the parameters of this pipeline step and the tracked input and output.\n",
    "\n",
    "Python script:\n",
    "\n",
    "**param** is used to declare the parameters of the corresponding Python 3 script and command. \n",
    "`#:param [type]? [param_name]: [description]?`\n",
    "\n",
    "DVC command:\n",
    "```\n",
    "[:dvc-[in|out][\\s{related_param}]?:[\\s{file_path}]?]*\n",
    "[:dvc-extra: {python_other_param}]?\n",
    "\n",
    "OR (for special cases it is possible to override the default behavior providing the full dvc command line)\n",
    "[:dvc-cmd: {full command line}]\n",
    "\n",
    "```\n",
    "**dvc-cmd** allows to provide the whole DVC command when it is not generic (this is an override of the default method). Here it is used because this pipeline step will run the same script twice with different parameters. describe the whole DVC command when it is not generic \n",
    "When it is used In this case, it is possible (and strongly recommended) to use the variable **$MLV_PY_CMD_PATH** to designate the Python command line path.\n",
    "\n",
    "\n",
    "To have a better understanding of those parameters, see the MLV-Tools [documentation](https://github.com/peopledoc/ml-versioning-tools) and have a look at the corresponding generated DVC command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# The Docstring below will be parsed by mlvtool to build the python script corresponding to this notebook with the proper parameters.\n",
    "\"\"\"\n",
    ":param str subset: Subset of data to load\n",
    ":param str data_home: Path to parent directory to cache file\n",
    ":param str output_path: Path to output file\n",
    ":dvc-cmd: dvc run -f $MLV_DVC_META_FILENAME -o ./poc/data/data_train.csv -o ./poc/data/data_test.csv\n",
    "                -d ./poc/data/20news-bydate_py3.pkz\n",
    "       \"$MLV_PY_CMD_PATH --subset train\n",
    "            --data-home ./poc/data --output-path ./poc/data/data_train.csv &&\n",
    "        $MLV_PY_CMD_PATH --subset test\n",
    "            --data-home ./poc/data --output-path ./poc/data/data_test.csv\"\n",
    "\"\"\"\n",
    "# Value of parameters for this Jupyter Notebook only\n",
    "# the notebook is places in ./poc/pipeline/notebooks\n",
    "subset = 'test'\n",
    "data_home = '../../data/'\n",
    "# Output:\n",
    "output_path = '../../data/data_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset=subset,\n",
    "                                      data_home=data_home,\n",
    "                                      download_if_missing=True,\n",
    "                                      remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No effect\n",
    "newsgroups_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No effect\n",
    "len(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No effect\n",
    "newsgroups_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(newsgroups_train.data, columns=['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target'] = newsgroups_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['targetnames'] = df_train['target'].apply(lambda n: newsgroups_train.target_names[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No effect\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(output_path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
